File Paths:
./README.md
./src/prompt_handling.rs
./src/git.rs
./src/validation.rs
./src/output.rs
./src/main.rs
./src/file_processing.rs
./src/cli.rs
./src/summary.rs

File Contents:


File Content of ./README.md:

# dirscribe

A CLI tool that collects and combines files with specific extensions from a directory into a single output. The output is copied to the clipboard by default.

## Features and Options

- Recursively traverse directory and filter by file extension
- Automatically applies .gitignore 
- Configure subpaths to include or exclude
- Filter by positive and/or negative keyword filters
- Only output diff, between commit ids or from a specified commit id to the current state
- Embed output in prompt template
- Write output to file
- Create summaries of file contents using LLM APIs
- Save summaries as comments on top of files
- Retrieve summaries from files with summaries added to them

## Installation

```bash
cargo install dirscribe
```

## Usage

Basic syntax:
```bash
dirscribe <comma_separated_suffixes_or_file_names_or_wildcard> [options]
```

Examples:
```bash
dirscribe md,py,Dockerfile
```

```bash
dirscribe "*"
```

### Demo (on Youtube)
[![Video showing how to use dirscribe](assets/public/thumbnail.jpg)](https://www.youtube.com/watch?v=rkXIZi1i3HI&t)

### Options

#### 'Deterministic' Processing options
- `--exclude-paths`: Comma-separated paths to exclude
- `--include-paths`: Comma-separated paths to include
- `--or-keywords`: Only include files containing at least one of these keywords
- `--and-keywords`: Only include files containing all of these keywords
- `--exclude-keywords`: Exclude files containing any of these keywords
- `--diff-only`: Only process files that have Git changes
- `--start-commit-id`: Starting commit ID for Git diff range (optional). If provided alone without end-commit-id, diffs from this commit to the current working directory
- `--end-commit-id`: Ending commit ID for Git diff range (optional). Must be used with start-commit-id
- `--prompt-template-path`: Path to a template file that will wrap the output. The template must contain the placeholder `${${CONTENT}$}$` where the collected content should be inserted
- `--output-path`: Path where the output file should be written. If not provided, output will be copied to clipboard
- `--dont-use-gitignore`: include files covered by .gitignore

#### LLM based options
- `--summarize`: Pass either file content or file diffs to LLM for summarization
- `--summarize-keywords`: Pass either file content or file diffs to LLM for summarization, and extract classes, functions and methods defined or used
- `--apply`: Write the LLM-generated summaries as multiline comments at the top of each file, to reduce duplicate work
- `--retrieve`: Retrieve summaries from files, after they were "applied" at a previous point

### Example with Diff Only

```bash
# Example using Git commit range
dirscribe rs,md \
  --diff-only \
  --start-commit-id abc123 \
  --end-commit-id def456
```

This will only process files that changed between commits abc123 and def456.

### Example with Summarize

```bash
dirscribe rs,md --summarize --apply
```

This will pass each file that was discovered to the Deepkseek or Anthropic API, or a locally running Ollama endpoint. The provider is set with the env variable `DIRSCRIBE_PROVIDER`, which can be set to `anthropic`, `deepseek`, `gemini` or `ollama`.

For each non-local provider, `PROVIDER_API_KEY` needs to be set.

The model used can be specified using `DIRSCRIBE_MODEL`.

The number of concurrent requests used can be set using `DIRSCRIBE_CONCURRENT_REQUESTS`.

### Example with Prompt Template

```bash
dirscribe rs,md \
  --exclude-paths src/core,src/temp \
  --or-keywords "TODO,FIXME" \
  --prompt-template-path "summarize-issues-to-address-prompt.txt"
```

## Output Format

The output is in this format:

```
File Paths:
/path/to/file1.txt
/path/to/file2.md

File Contents:
File: /path/to/file1.txt
[Contents of file1.txt]

File: /path/to/file2.md
[Contents of file2.md]
```

If a prompt template path is specified, this output will be embedded in that template for the final output.

## Template

You can specify a template to embed the output in. The template should be a txt file that contains the string "${${CONTENT}$}$" (without quotation marks), and that string will be replaced with the output as shown above.

## License

MIT License


File Content of ./src/prompt_handling.rs:

use std::collections::HashMap;

pub fn load_prompts(_dir: &str) -> std::io::Result<HashMap<String, String>> {
    let mut prompts = HashMap::new();
    
    // Include prompt files at compile time
    prompts.insert(
        "summary-0.2".to_string(),
        include_str!("../prompts/summary-0.2.txt").to_string()
    );

    prompts.insert(
        "summary-keywords-0.1".to_string(),
        include_str!("../prompts/summary-keywords-0.1.txt").to_string()
    );
    
    prompts.insert(
        "summary-diff-0.1".to_string(),
        include_str!("../prompts/summary-diff-0.1.txt").to_string()
    );
    
    Ok(prompts)
}

File Content of ./src/git.rs:

use std::io;
use std::path::{Path, PathBuf};
use git2::{Repository, Tree, Diff, DiffFormat};

pub fn get_diff_list(
    repo: &Repository,
    start_commit_id: Option<&str>,
    end_commit_id: Option<&str>,
) -> io::Result<Vec<PathBuf>> {
    let mut diff_list = Vec::new();
    
    // Helper function to get tree from commit ID
    let get_tree = |commit_id: &str| -> io::Result<Tree> {
        repo.revparse_single(commit_id)
            .map_err(|e| io::Error::new(io::ErrorKind::Other, e.message().to_string()))?
            .peel_to_commit()
            .map_err(|e| io::Error::new(io::ErrorKind::Other, e.message().to_string()))?
            .tree()
            .map_err(|e| io::Error::new(io::ErrorKind::Other, e.message().to_string()))
    };

    // Get the diff based on provided arguments
    let diff = match (start_commit_id, end_commit_id) {
        // Both None: compare working directory with HEAD
        (None, None) => {
            let head_tree = repo.head()
                .map_err(|e| io::Error::new(io::ErrorKind::Other, e.message().to_string()))?
                .peel_to_tree()
                .map_err(|e| io::Error::new(io::ErrorKind::Other, e.message().to_string()))?;
            
            repo.diff_tree_to_workdir_with_index(
                Some(&head_tree),
                None
            )
        },
        // Only old_commit provided: compare that commit with working directory
        (Some(old_id), None) => {
            let old_tree = get_tree(old_id)?;
            repo.diff_tree_to_workdir_with_index(
                Some(&old_tree),
                None
            )
        },
        // Both provided: compare the two commits directly
        (Some(old_id), Some(new_id)) => {
            let old_tree = get_tree(old_id)?;
            let new_tree = get_tree(new_id)?;
            repo.diff_tree_to_tree(
                Some(&old_tree),
                Some(&new_tree),
                None
            )
        },
        // Invalid case: old None but new Some - treat as comparing HEAD to new commit
        (None, Some(new_id)) => {
            let head_tree = repo.head()
                .map_err(|e| io::Error::new(io::ErrorKind::Other, e.message().to_string()))?
                .peel_to_tree()
                .map_err(|e| io::Error::new(io::ErrorKind::Other, e.message().to_string()))?;
            let new_tree = get_tree(new_id)?;
            repo.diff_tree_to_tree(
                Some(&head_tree),
                Some(&new_tree),
                None
            )
        }
    }.map_err(|e| io::Error::new(io::ErrorKind::Other, e.message().to_string()))?;
    
    // Collect changed files
    diff.foreach(
        &mut |delta, _| {
            if let Some(new_file) = delta.new_file().path() {
                diff_list.push(new_file.to_path_buf());
            }
            true
        },
        None,
        None,
        None,
    ).map_err(|e| io::Error::new(io::ErrorKind::Other, e.message().to_string()))?;
    
    Ok(diff_list)
}

pub fn get_diff_str(diff: &Diff) -> io::Result<String> {
    let mut diff_output = Vec::new();
    
    diff.print(DiffFormat::Patch, |_delta, _hunk, line| {
        let origin = line.origin();
        
        // For header lines (not +, -, or space), just add the content
        if origin != '+' && origin != '-' && origin != ' ' {
            diff_output.extend_from_slice(line.content());
        } else {
            // For actual diff lines, add the origin character and content
            diff_output.push(origin as u8);
            diff_output.extend_from_slice(line.content());
        }
        true
    }).map_err(|e| io::Error::new(io::ErrorKind::Other, e.message().to_string()))?;

    let output = String::from_utf8(diff_output)
        .map_err(|e| io::Error::new(io::ErrorKind::InvalidData, e))?;
    
    if output.is_empty() {
        return Ok("No changes detected".to_string());
    }
    
    Ok(output)
}

pub fn filter_diff_for_file(diff_str: &str, file_path: &Path) -> String {
    let lines: Vec<&str> = diff_str.lines().collect();
    let mut result = Vec::new();
    let mut current_file_section = false;
    // Get just the filename component
    let file_name = file_path.file_name()
        .map(|s| s.to_string_lossy())
        .unwrap_or_default();

    for line in lines {
        if line.starts_with("diff --git") {
            // Check if this section is for our file
            current_file_section = line.contains(&*file_name);
            if current_file_section {
                result.push(line);
            }
        } else if current_file_section {
            // Keep adding lines until we hit the next diff section
            if line.starts_with("diff --git") {
                break;
            }
            result.push(line);
        }
    }

    result.join("\n")
}

File Content of ./src/validation.rs:

use std::path::{Path, PathBuf};
use crate::cli::Cli;
use git2::Repository;
use anyhow::Result;
use std::error::Error;
use std::fmt;

#[derive(Debug)]
pub struct ValidationError(String);

impl fmt::Display for ValidationError {
    fn fmt(&self, f: &mut fmt::Formatter) -> fmt::Result {
        write!(f, "{}", self.0)
    }
}

impl Error for ValidationError {}

impl From<String> for ValidationError {
    fn from(msg: String) -> Self {
        ValidationError(msg)
    }
}

impl From<&str> for ValidationError {
    fn from(msg: &str) -> Self {
        ValidationError(msg.to_string())
    }
}

pub fn validate_cli_args(cli: &Cli) -> Result<()> {
    // Validate suffixes
    validate_suffixes(&cli.suffixes)?;

    // Validate paths
    if let Some(template_path) = &cli.prompt_template_path {
        validate_template_path(template_path)?;
    }

    if let Some(output_path) = &cli.output_path {
        validate_output_path(output_path)?;
    }

    // Validate git-related arguments
    validate_git_args(
        cli.diff_only,
        &cli.start_commit_id,
        &cli.end_commit_id,
    )?;

    // Validate keywords
    validate_keywords(&cli.or_keywords, "or_keywords")?;
    validate_keywords(&cli.and_keywords, "and_keywords")?;
    validate_keywords(&cli.exclude_keywords, "exclude_keywords")?;

    // Validate exclude/include paths
    validate_path_filters(
        &cli.exclude_paths,
        &cli.include_paths,
    )?;

    if cli.apply && (!cli.summarize && !cli.summarize_keywords){
        return Err(ValidationError("--apply can only be used with --summarize or --summarize_keywords".into()).into());
    }

    if cli.retrieve && (!cli.summarize && !cli.summarize_keywords) {
        return Err(ValidationError("--retrieve can only be used with --summarize or --summarize_keywords".into()).into());
    }

    if cli.apply && cli.diff_only {
        return Err(ValidationError("--apply cannot be used with --diff-only".into()).into());
    }

    if cli.diff_only && cli.retrieve {
        return Err(ValidationError("--retrieve is not available with --diff-only".into()).into());
    }

    if cli.apply && cli.retrieve {
        return Err(ValidationError("--apply and --retrieve cannot be used together".into()).into());
    }


    Ok(())
}

fn validate_suffixes(suffixes: &str) -> Result<(), ValidationError> {
    if suffixes.is_empty() {
        return Err(ValidationError("Suffixes cannot be empty".to_string()));
    }

    if suffixes == "*" {
        return Ok(());
    }

    let parts: Vec<&str> = suffixes.split(',').collect();
    
    for suffix in parts {
        if suffix.is_empty() {
            return Err("Empty suffix found after splitting".into());
        }

        if !suffix.chars().all(|c| c.is_alphanumeric()) {
            return Err(format!("Invalid suffix '{}': must be alphanumeric", suffix).into());
        }

        if suffix.len() > 10 {
            return Err(format!("Suffix '{}' exceeds maximum length of 10", suffix).into());
        }
    }

    Ok(())
}

fn validate_template_path(path: &str) -> Result<(), ValidationError> {
    let path = Path::new(path);
    
    if !path.exists() {
        return Err(format!("Template file does not exist: {}", path.display()).into());
    }

    if !path.is_file() {
        return Err(format!("Template path is not a file: {}", path.display()).into());
    }

    // Check file size (e.g., max 1MB)
    if let Ok(metadata) = path.metadata() {
        if metadata.len() > 100_000_000 {
            return Err("Template file is too large (max 100MB)".into());
        }
    }

    Ok(())
}

fn validate_output_path(path: &str) -> Result<(), ValidationError> {
    let path = Path::new(path);
    
    // Check if path points to a directory
    if path.is_dir() {
        return Err(format!("Output path is a directory: {}", path.display()).into());
    }

    Ok(())
}

fn validate_git_args(
    diff_only: bool,
    start_commit: &Option<String>,
    end_commit: &Option<String>,
) -> Result<(), ValidationError> {

    if diff_only && start_commit.is_none() {
        return Err("--start-commit-id must be provided when using --diff-only".into());
    }
    if start_commit.is_some() && !diff_only {
        return Err("--diff-only must be set when using --start-commit-id".into());
    }
    if end_commit.is_some() && !diff_only {
        return Err("--diff-only must be set when using --end-commit-id".into());
    }
    if end_commit.is_some() && start_commit.is_none() {
        return Err("--start-commit-id must be set when using --end-commit-id".into());
    }

    // Verify we're in a git repository
    if diff_only {
        let repo = match Repository::open(".") {
            Ok(repo) => repo,
            Err(_) => return Err("Not a git repository".into()),
        };

        if let Some(start) = start_commit {
            validate_commit(&repo, start, "start_commit_id")?;
        }

        if let Some(end) = end_commit {
            validate_commit(&repo, end, "end_commit_id")?;
        }

        // If both commits provided, verify start is ancestor of end
        if let (Some(start), Some(end)) = (start_commit, end_commit) {
            let start_commit = repo
                .revparse_single(start)
                .map_err(|_| format!("Invalid start commit: {}", start))?
                .peel_to_commit()
                .map_err(|_| "Failed to parse start commit".to_string())?;

            let end_commit = repo
                .revparse_single(end)
                .map_err(|_| format!("Invalid end commit: {}", end))?
                .peel_to_commit()
                .map_err(|_| "Failed to parse end commit".to_string())?;

            if !repo.graph_descendant_of(end_commit.id(), start_commit.id())
                .map_err(|_| "Failed to check commit relationship".to_string())? {
                return Err("start_commit_id must be an ancestor of end_commit_id".into());
            }
        }

    }
    Ok(())
}

fn validate_commit(repo: &Repository, commit_id: &str, arg_name: &str) -> Result<(), ValidationError> {
    match repo.revparse_single(commit_id) {
        Ok(obj) => {
            if obj.as_commit().is_none() {
                return Err(format!("{} is not a valid commit", arg_name).into());
            }
        }
        Err(_) => {
            return Err(format!("Invalid {}: {}", arg_name, commit_id).into());
        }
    }
    Ok(())
}

fn validate_keywords(keywords: &Option<String>, field_name: &str) -> Result<(), ValidationError> {
    if let Some(keywords) = keywords {
        let parts: Vec<&str> = keywords.split(',').collect();
        
        for keyword in parts {
            if keyword.is_empty() {
                return Err(format!("Empty keyword found in {}", field_name).into());
            }

            if keyword.len() > 100 {
                return Err(format!("Keyword in {} exceeds maximum length of 100", field_name).into());
            }

            // Check for invalid characters (optional - adjust as needed)
            if keyword.chars().any(|c| !c.is_ascii()) {
                return Err(format!("Non-ASCII characters found in {} keyword: {}", field_name, keyword).into());
            }
        }
    }
    Ok(())
}

fn validate_path_filters(
    exclude_paths: &Option<String>,
    include_paths: &Option<String>,
) -> Result<(), ValidationError> {
    let mut all_paths = Vec::new();

    // Helper function to process paths
    let process_paths = |paths_str: &str, is_exclude: bool| -> Result<Vec<PathBuf>, ValidationError> {
        let paths: Vec<PathBuf> = paths_str
            .split(',')
            .filter(|s| !s.is_empty())
            .map(PathBuf::from)
            .collect();

        for path in &paths {
            // Normalize path
            let normalized = path.canonicalize().map_err(|_| {
                format!("{} path does not exist: {}", 
                    if is_exclude { "Exclude" } else { "Include" },
                    path.display()
                )
            })?;

            // Verify path is within project directory
            let current_dir = std::env::current_dir().map_err(|_| 
                "Failed to get current directory".to_string()
            )?;
            
            if !normalized.starts_with(current_dir) {
                return Err(format!("Path is outside project directory: {}", path.display()).into());
            }
        }

        Ok(paths)
    };

    if let Some(exclude) = exclude_paths {
        all_paths.extend(process_paths(exclude, true)?);
    }

    if let Some(include) = include_paths {
        let include_paths = process_paths(include, false)?;
        
        // Check for conflicts between include and exclude paths
        for include_path in &include_paths {
            if all_paths.iter().any(|p| include_path.starts_with(p)) {
                return Err(format!(
                    "Include path conflicts with exclude path: {}", 
                    include_path.display()
                ).into());
            }
        }
        
        all_paths.extend(include_paths);
    }

    Ok(())
}

File Content of ./src/output.rs:

use std::fs;
use anyhow::{Result, bail};
use clipboard::{ClipboardContext, ClipboardProvider};

pub fn write_to_clipboard(content: &str) -> Result<()> {
    let mut ctx: ClipboardContext = ClipboardProvider::new()
        .map_err(|e| anyhow::anyhow!("Failed to create clipboard context: {}", e))?;
    
    ctx.set_contents(content.to_owned())
        .map_err(|e| anyhow::anyhow!("Failed to set clipboard contents: {}", e))?;
    
    Ok(())
}

pub fn process_with_template(content: &str, template_path: &str) -> Result<String> {
    // Read the template file
    let template = fs::read_to_string(template_path)
        .map_err(|e| anyhow::anyhow!("Failed to read template file: {}", e))?;

    // Check for the required placeholder
    if !template.contains("${${CONTENT}$}$") {
        bail!("Template file must contain the placeholder '${{${{CONTENT}}$}}$'");
    }

    // Replace the placeholder with the content
    Ok(template.replace("${${CONTENT}$}$", content))
}

File Content of ./src/main.rs:

use std::fs::File;
mod cli;
mod git;
mod file_processing;
mod output;
mod prompt_handling;
mod summary; 
mod validation;
use cli::Cli;
use file_processing::process_directory;
use output::{write_to_clipboard, process_with_template};
use clap::Parser;
use validation::validate_cli_args;
use anyhow::{Result, Context};
use std::io::Write;
use std::path::PathBuf;
use prompt_handling::load_prompts;



#[tokio::main]
async fn main() -> Result<()> {
    let cli = Cli::parse();

    assert!(
        cli.suffixes == "*" || !cli.suffixes.chars().any(|s| s == '*'),
        "\"*\" can only be used alone, file extensions are specified without wildcard, like 'py,toml,js'"
    );

    if let Err(e) = validate_cli_args(&cli) {
        eprintln!("Error: {}", e);
        std::process::exit(1);
    }

    
    let suffixes: Vec<String> = cli.suffixes.split(',').map(String::from).collect();
    
    let exclude_paths: Vec<PathBuf> = match cli.exclude_paths {
        Some(s) => {
            if s.contains(',') {
                s.split(',').map(PathBuf::from).collect()
            } else {
                vec![PathBuf::from(s)]
            }
        }
        None => Vec::new()
    };

    let include_paths: Vec<PathBuf> = match cli.include_paths {
        Some(s) => {
            if s.contains(',') {
                s.split(',').map(PathBuf::from).collect()
            } else {
                vec![PathBuf::from(s)]
            }
        }
        None => Vec::new()
    };

    let or_keywords: Vec<String> = cli.or_keywords
        .map(|s| if s.contains(',') {
            s.split(',').map(String::from).collect()
        } else {
            vec![s]
        })
        .unwrap_or_default();

    let and_keywords: Vec<String> = cli.and_keywords
        .map(|s| if s.contains(',') {
            s.split(',').map(String::from).collect()
        } else {
            vec![s]
        })
        .unwrap_or_default();

    let exclude_keywords: Vec<String> = cli.exclude_keywords
        .map(|s| if s.contains(',') {
            s.split(',').map(String::from).collect()
        } else {
            vec![s]
        })
        .unwrap_or_default();

    // Read the file contents into a String
    let summarize_prompt_templates = load_prompts("prompts").context("Failed to load prompt templates")?;
    // Process directory and get the content string
    let content = process_directory(
        ".",
        &suffixes,
        cli.dont_use_gitignore,
        cli.summarize,
        cli.summarize_keywords,
        summarize_prompt_templates,
        cli.apply,
        cli.retrieve,
        cli.diff_only,
        &exclude_paths,
        &include_paths,
        &or_keywords,
        &and_keywords,
        &exclude_keywords,
        cli.start_commit_id.as_deref(),
        cli.end_commit_id.as_deref()
    ).await?;

    let final_content = if let Some(template_path) = cli.prompt_template_path {
        process_with_template(&content, &template_path)?
    } else {
        content
    };


    if let Some(output_path) = cli.output_path {
        let mut output_file = File::create(&output_path)?;
        output_file.write_all(final_content.as_bytes())?;
        println!("Successfully processed directory and written output to {}", output_path);
    } else {
        write_to_clipboard(&final_content)?;
        println!("Successfully processed directory and copied output to clipboard");
    };
    Ok(())
}





File Content of ./src/file_processing.rs:

use std::fs;
use std::io::{self, Write, Cursor};
use std::path::{Path, PathBuf};
use anyhow::Context;
use ignore::WalkBuilder;
use std::collections::HashMap;
use git2::{Repository, Tree};
use chrono::Local;
use crate::git::{get_diff_list, get_diff_str, filter_diff_for_file};
use crate::summary::{get_summaries, check_summary};


pub async fn process_directory(
    dir_path: &str,
    suffixes: &[String],
    dont_use_gitignore: bool,
    summarize: bool,
    summarize_keywords: bool,
    summarize_prompt_templates: HashMap<String, String>,
    apply: bool,
    retrieve: bool,
    diff_only: bool,
    exclude_paths: &[PathBuf],
    include_paths: &[PathBuf],
    or_keywords: &[String],
    and_keywords: &[String],
    exclude_keywords: &[String],
    start_commit_id: Option<&str>,
    end_commit_id: Option<&str>
) -> anyhow::Result<String> {
    let mut output = Cursor::new(Vec::new());
    let dir_path = Path::new(dir_path);
    
    let repo = if diff_only {
        Some(Repository::open(dir_path).context("Failed to open git repository")?)
    } else {
        None
    };

    if !dir_path.exists() {
        return Err(anyhow::anyhow!("Directory not found"));
    }

    let mut diff_list = Vec::new();
    if diff_only {
        if let Some(repo) = &repo {
            diff_list = get_diff_list(repo, start_commit_id, end_commit_id)?;
        }
    }

    // First, collect all valid file paths
    let mut valid_files = Vec::new();
    
    let walker = WalkBuilder::new(dir_path)
        .hidden(false)
        .git_ignore(!dont_use_gitignore)
        .build();

    for result in walker {
        match result {
            Ok(entry) => {
                let path = entry.path();
                
                // Skip if diff_only is true and path is not in diff_list
                if diff_only {
                    if let Ok(relative_path) = path.strip_prefix(dir_path) {
                        if !diff_list.contains(&relative_path.to_path_buf()) {
                            continue;
                        }
                    }
                }

                let should_include = if path.is_dir() {
                    false
                } else if suffixes.contains(&"*".to_string()) {
                    // If wildcard is specified, check if it's a text-like file
                    is_likely_text_file(path)
                } else if let Some(file_suffix) = path.extension() {
                    suffixes.iter().any(|s| s == file_suffix.to_str().unwrap_or(""))
                } else {
                    if let Some(filename) = path.file_name() {
                        suffixes.iter().any(|s| s == filename.to_str().unwrap_or(""))
                    } else {
                        false
                    }
                };

                if should_include {
                    // Get relative path from base directory
                    if let Ok(relative_path) = path.strip_prefix(dir_path) {
                        let relative_path_str = relative_path.to_string_lossy();
                        
                        // Skip if path matches any exclude pattern
                        if exclude_paths.iter().any(|excluded| 
                            relative_path_str.starts_with(&excluded.to_string_lossy().as_ref())
                        ) {
                            continue;
                        }
                        
                        // Skip if include patterns exist and path doesn't match any
                        if !include_paths.is_empty() {
                            let is_included = include_paths.iter().any(|included|
                                relative_path_str.starts_with(&included.to_string_lossy().as_ref())
                            );
                            if !is_included {
                                continue;
                            }
                        }

                        // Check keyword filters before adding to valid files
                        if check_for_keywords(
                            &path.to_path_buf(),
                            or_keywords,
                            and_keywords,
                            exclude_keywords,
                        )? {
                            valid_files.push(path.to_path_buf());
                        }
                    }
                }
            }
            Err(err) => eprintln!("Error walking directory: {}", err),
        }
    }

    // Write all file paths at the top
    writeln!(output, "File Paths:")?;
    for file_path in &valid_files {
        writeln!(output, "{}", file_path.display())?;
    }
    writeln!(output)?;
    if !summarize && !summarize_keywords {
        writeln!(output, "File Contents:")?;
    } else {
        writeln!(output, "File Summaries:")?;
    }
    writeln!(output)?;

    let file_contents: HashMap<String, String> = valid_files
        .iter()
        .filter_map(|file_path| {
            let path_string = file_path.to_string_lossy().into_owned();
            match process_file(
                file_path,
                diff_only,
                repo.as_ref(),
                start_commit_id,
                end_commit_id
            ) {
                Ok(content) => Some((path_string, content)),
                Err(e) => {
                    eprintln!("Error processing file {}: {}", file_path.display(), e);
                    None
                }
            }
        })
        .collect();

    // Generate output string maintaining file path order
    let result = if summarize | summarize_keywords {
        let valid_file_strings: Vec<String> = valid_files.iter()
            .map(|path| path.to_string_lossy().into_owned())
            .collect();
        

        let suffix_map = create_comment_map();

        let summaries = if !diff_only {
            if !retrieve {
                if summarize {
                    get_summaries(valid_file_strings.clone(), file_contents.clone(), summarize_prompt_templates["summary-0.2"].clone(), suffix_map.clone(), diff_only).await?
                } else { // if summarize_keywords 
                    get_summaries(valid_file_strings.clone(), file_contents.clone(), summarize_prompt_templates["summary-keywords-0.1"].clone(), suffix_map.clone(), diff_only).await?
                }
            } else {
                get_summaries_from_files(valid_file_strings.clone(), file_contents.clone())
            }
        } else {
            get_summaries(valid_file_strings, file_contents.clone(), summarize_prompt_templates["summary-diff-0.1"].clone(), suffix_map.clone(), diff_only).await?
        };
        
        if apply && !diff_only {
            // Zip together the files and their summaries
            for (file_path, summary) in valid_files.iter().zip(summaries.iter()) {
                if let Err(e) = write_summary_to_file(file_path, summary, suffix_map.clone()) {
                    eprintln!("Error writing summary to {}: {}", file_path.display(), e);
                }
            }
            
        }
    
        // Use the original valid_files order
        valid_files.iter().zip(summaries.iter())
            .map(|(file, summary)| {
                format!("\nSummary of {}:\n\n{}\n", file.display(), summary)
            })
            .collect::<Vec<String>>()
            .join("")
    } else if diff_only {
        valid_files.iter()
            .filter_map(|file| {
                let path_string = file.to_string_lossy().into_owned();
                file_contents.get(&path_string)
                    .map(|content| format!("\nDiff of {}:\n\n{}\n", file.display(), content))
            })
            .collect::<Vec<String>>()
            .join("")
    } else {
        valid_files.iter()
            .filter_map(|file| {
                let path_string = file.to_string_lossy().into_owned();
                file_contents.get(&path_string)
                    .map(|content| format!("\nFile Content of {}:\n\n{}\n", file.display(), content))
            })
            .collect::<Vec<String>>()
            .join("")
    };

    write!(output, "{}", result)?;
    
    String::from_utf8(output.into_inner())
        .map_err(|e| io::Error::new(io::ErrorKind::InvalidData, e))
        .map_err(Into::into)
}

fn check_prefix(s: &str) -> bool {
    let lines: Vec<_> = s.split('\n').collect();
    if lines.is_empty() { return true; }
    let first = lines[0].trim_start();
    let is_hash = first.starts_with('#');
    lines.iter().all(|l| l.trim_start().starts_with(if is_hash { "#" } else { "//" }))
}

fn get_summaries_from_files(
    valid_files: Vec<String>, 
    file_contents: HashMap<String, String>
) -> Vec<String> {
    let mut summaries = Vec::new();

    for file_path in valid_files {
        let content = file_contents.get(&file_path).unwrap_or(&String::new()).clone();
        
        let summary =  filter_dirscribe_sections(&content, false);
        summaries.push(summary)
    }

    summaries
}

pub fn filter_dirscribe_sections(content: &str, exclude: bool) -> String {
    let lines: Vec<&str> = content.lines().collect();
    if lines.is_empty() {
        return String::new();
    }

    // Create iterator tuples with (previous, current, next) lines
    let with_context = (0..lines.len()).map(|i| {
        let prev = if i > 0 { Some(lines[i - 1]) } else { None };
        let current = lines[i];
        let next = if i < lines.len() - 1 { Some(lines[i + 1]) } else { None };
        (prev, current, next)
    });
    let mut line_number = 0;
    let mut in_dirscribe = false;
    let filtered_lines: Vec<&str> = with_context
        .filter(|(prev, _, next)| {
            line_number += 1;

            if let Some(next_line) = next {
                if line_number < 3 && next_line.contains("[DIRSCRIBE]") {
                    in_dirscribe = true;
                    if exclude {
                        return false;
                    } else {
                        return true
                    }
                }
            }

            if let Some(prev_line) = prev {
                if in_dirscribe && prev_line.contains("[/DIRSCRIBE]"){
                    in_dirscribe = false;
                    if exclude {
                        return false;
                    } else {
                        return true
                    }
                }
            }

            if exclude {
                !in_dirscribe
            } else {
                in_dirscribe
            }
        })
        .map(|(_, current, _)| current)
        .collect();

    filtered_lines.join("\n")
}


fn insert_timestamp(input: &str) -> String {
    let mut lines: Vec<&str> = input.lines().collect();
    let timestamp = Local::now().to_rfc3339();
    lines.insert(lines.len() - 2, &timestamp);
    lines.join("\n")
}

pub fn write_summary_to_file(file_path: &Path, summary: &str, suffix_map: HashMap<&'static str, Vec<(&'static str, &'static str)>>) -> anyhow::Result<()> {
    if check_summary(file_path, summary, &suffix_map) | check_prefix(summary) {
        let content = fs::read_to_string(file_path)?;    
        let processed_content = filter_dirscribe_sections(&content, true);
        let summary_ts = insert_timestamp(summary);
        let summary_block = format!("{}\n", summary_ts);
        let new_content = summary_block + &processed_content;
        fs::write(file_path, new_content)?;
        Ok(())
    } else {
        return Err(anyhow::anyhow!("Summary is not a correctly formatted comment. (doesn't start with a comment char on every line or doesn't have starting or ending line with multi line comment enclosure)"));

    }
}


pub fn process_file(
    file_path: &PathBuf,
    diff_only: bool,
    repo: Option<&Repository>,
    start_commit_id: Option<&str>,
    end_commit_id: Option<&str>
) -> io::Result<String> {
    let _relative_path = if let Some(repo) = repo {
        let repo_workdir = repo.workdir().ok_or_else(|| {
            io::Error::new(io::ErrorKind::Other, "Could not get repository working directory")
        })?;
        
        let full_path = fs::canonicalize(file_path)?;
        let relative_path = full_path.strip_prefix(fs::canonicalize(repo_workdir)?)
            .map_err(|_| io::Error::new(io::ErrorKind::Other, "File not in repository"))?;
            
        relative_path.to_path_buf()
    } else {
        file_path.clone()
    };

    let contents = if !diff_only {
        fs::read_to_string(file_path)?
    } else {
        if let Some(repo) = repo {
            let get_tree = |commit_id: &str| -> io::Result<Tree> {
                repo.revparse_single(commit_id)
                    .map_err(|e| io::Error::new(io::ErrorKind::Other, e.message().to_string()))?
                    .peel_to_commit()
                    .map_err(|e| io::Error::new(io::ErrorKind::Other, e.message().to_string()))?
                    .tree()
                    .map_err(|e| io::Error::new(io::ErrorKind::Other, e.message().to_string()))
            };

            let diff = match (start_commit_id, end_commit_id) {
                (None, None) => {
                    let head_tree = repo.head()
                        .map_err(|e| io::Error::new(io::ErrorKind::Other, e.message().to_string()))?
                        .peel_to_tree()
                        .map_err(|e| io::Error::new(io::ErrorKind::Other, e.message().to_string()))?;
                    
                    repo.diff_tree_to_workdir_with_index(Some(&head_tree), None)
                },
                (Some(old_id), None) => {
                    let old_tree = get_tree(old_id)?;
                    repo.diff_tree_to_workdir_with_index(Some(&old_tree), None)
                },
                (Some(old_id), Some(new_id)) => {
                    let old_tree = get_tree(old_id)?;
                    let new_tree = get_tree(new_id)?;
                    repo.diff_tree_to_tree(Some(&old_tree), Some(&new_tree), None)
                },
                (None, Some(new_id)) => {
                    let head_tree = repo.head()
                        .map_err(|e| io::Error::new(io::ErrorKind::Other, e.message().to_string()))?
                        .peel_to_tree()
                        .map_err(|e| io::Error::new(io::ErrorKind::Other, e.message().to_string()))?;
                    let new_tree = get_tree(new_id)?;
                    repo.diff_tree_to_tree(Some(&head_tree), Some(&new_tree), None)
                }
            }.map_err(|e| io::Error::new(io::ErrorKind::Other, e.message().to_string()))?;

            let diff_str = get_diff_str(&diff)?;
            filter_diff_for_file(&diff_str, file_path) // Removed unnecessary semicolon
        } else {
            String::new() // Added else branch for when repo is None
        }
    };

    Ok(contents)
}


pub fn check_for_keywords(
    file_path: &PathBuf,
    or_keywords: &[String],
    and_keywords: &[String],
    exclude_keywords: &[String],
) -> io::Result<bool> {


    let contents = fs::read_to_string(file_path)?;

    // Check exclude keywords - skip if any are present
    if exclude_keywords.iter().any(|keyword| contents.contains(keyword)) {
        return Ok(false);
    }
    
    // Check OR keywords - at least one must be present
    if !or_keywords.is_empty() {
        let contains_or_keyword = or_keywords.iter().any(|keyword| contents.contains(keyword));
        if !contains_or_keyword {
            return Ok(false);
        }
    }

    // Check AND keywords - all must be present
    if !and_keywords.is_empty() {
        let contains_all_keywords = and_keywords.iter().all(|keyword| contents.contains(keyword));
        if !contains_all_keywords {
            return Ok(false);
        }
    }

    Ok(true)
}

// Add this function at the top of file_processing.rs
fn is_likely_text_file(path: &Path) -> bool {
    // Common text file extensions
    const TEXT_EXTENSIONS: &[&str] = &[
        // Programming languages
        "rs", "py", "js", "ts", "java", "c", "cpp", "h", "hpp", "cs", "go", "rb", "php", "swift",
        "kt", "scala", "sh", "bash", "pl", "r", "sql", "m", "mm",
        // Web
        "html", "htm", "css", "scss", "sass", "less", "xml", "svg",
        // Data formats
        "json", "yaml", "yml", "toml", "ini", "conf", "config",
        // Documentation
        "md", "markdown", "txt", "rtf", "rst", "asciidoc", "adoc",
        // Config files
        "gitignore", "env", "dockerignore", "editorconfig",
        // Build files
        "cmake", "make", "mak", "gradle",
    ];

    // Always consider files without extension that are commonly text
    const EXTENSION_LESS_TEXT_FILES: &[&str] = &[
        "Dockerfile", "Makefile", "README", "LICENSE", "Cargo.lock", "package.json",
        ".gitignore", ".env", ".dockerignore", ".editorconfig"
    ];

    if let Some(file_name) = path.file_name().and_then(|n| n.to_str()) {
        // Check extension-less files first
        if EXTENSION_LESS_TEXT_FILES.contains(&file_name) {
            return true;
        }
    }

    // Check file extension
    if let Some(ext) = path.extension().and_then(|e| e.to_str()) {
        if TEXT_EXTENSIONS.contains(&ext.to_lowercase().as_str()) {
            return true;
        }
    }

    // For files without extension or unknown extensions, try to read a small sample
    // and check if it contains only valid UTF-8 text
    if let Ok(file) = std::fs::File::open(path) {
        use std::io::Read;
        let mut buffer = [0u8; 1024];
        let mut handle = file;
        
        // Read first 1024 bytes
        if handle.read(&mut buffer).is_ok() {
            // Check if content is valid UTF-8
            return String::from_utf8(buffer.to_vec()).is_ok();
        }
    }

    false
}



fn create_comment_map() -> HashMap<&'static str, Vec<(&'static str, &'static str)>> {
    let mut map = HashMap::new();
    
    // Helper function to insert comment styles
    let mut insert = |ext: &'static str, comments: Vec<(&'static str, &'static str)>| {
        map.insert(ext, comments);
    };

    // ActionScript
    insert("as", vec![("/*", "*/")]);
    
    // Ada
    insert("ada", vec![("/*", "*/")]);
    insert("adb", vec![("/*", "*/")]);
    insert("ads", vec![("/*", "*/")]);
    
    // AppleScript
    insert("scpt", vec![("(*", "*)")]);
    insert("applescript", vec![("(*", "*)")]);
    
    // Assembly
    insert("asm", vec![("/*", "*/")]);
    insert("s", vec![("/*", "*/")]);
    
    // AWK
    insert("awk", vec![("/*", "*/")]);
    
    // Bash
    insert("sh", vec![(":'", "'"), ("#", "\n")]);
    insert("bash", vec![(":'", "'"), ("#", "\n")]);
    
    // C
    insert("c", vec![("/*", "*/"), ("//", "\n")]);
    insert("h", vec![("/*", "*/"), ("//", "\n")]);
    
    // C#
    insert("cs", vec![("/*", "*/"), ("//", "\n")]);
    
    // C++
    let cpp_comments = vec![("/*", "*/"), ("//", "\n")];
    insert("cpp", cpp_comments.clone());
    insert("hpp", cpp_comments.clone());
    insert("cc", cpp_comments.clone());
    insert("hh", cpp_comments.clone());
    insert("cxx", cpp_comments.clone());
    insert("hxx", cpp_comments.clone());
    
    // COBOL
    insert("cob", vec![("/*", "*/")]);
    insert("cbl", vec![("/*", "*/")]);
    
    // CoffeeScript
    insert("coffee", vec![("###", "###"), ("#", "\n")]);
    
    // CSS
    insert("css", vec![("/*", "*/")]);
    
    // D
    insert("d", vec![("/*", "*/"), ("//", "\n")]);
    
    // Dart
    insert("dart", vec![("/*", "*/"), ("//", "\n")]);
    
    // Delphi/Pascal
    insert("pas", vec![("{", "}"), ("(*", "*)")]);
    insert("dpr", vec![("{", "}"), ("(*", "*)")]);
    
    // Elixir
    insert("ex", vec![("#=", "=#"), ("#", "\n")]);
    insert("exs", vec![("#=", "=#"), ("#", "\n")]);
    
    // Erlang
    insert("erl", vec![("%%%", "%%%"), ("%", "\n")]);
    insert("hrl", vec![("%%%", "%%%"), ("%", "\n")]);
    
    // F#
    insert("fs", vec![("(*", "*)"), ("//", "\n")]);
    insert("fsx", vec![("(*", "*)"), ("//", "\n")]);
    
    // Go
    insert("go", vec![("/*", "*/"), ("//", "\n")]);
    
    // Groovy
    insert("groovy", vec![("/*", "*/"), ("//", "\n")]);
    insert("gvy", vec![("/*", "*/"), ("//", "\n")]);
    
    // Haskell
    insert("hs", vec![("{-", "-}"), ("--", "\n")]);
    insert("lhs", vec![("{-", "-}"), ("--", "\n")]);
    
    // HTML/XML
    let xml_comments = vec![("<!--", "-->")];
    insert("html", xml_comments.clone());
    insert("htm", xml_comments.clone());
    insert("xml", xml_comments.clone());
    insert("xsl", xml_comments.clone());
    insert("xsd", xml_comments.clone());
    
    // Java
    insert("java", vec![("/*", "*/"), ("//", "\n")]);
    
    // JavaScript
    insert("js", vec![("/*", "*/"), ("//", "\n")]);
    insert("mjs", vec![("/*", "*/"), ("//", "\n")]);
    
    // Julia
    insert("jl", vec![("#=", "=#"), ("#", "\n")]);
    
    // Kotlin
    insert("kt", vec![("/*", "*/"), ("//", "\n")]);
    insert("kts", vec![("/*", "*/"), ("//", "\n")]);
    
    // LISP
    insert("lisp", vec![("#|", "|#"), (";", "\n")]);
    insert("lsp", vec![("#|", "|#"), (";", "\n")]);
    insert("cl", vec![("#|", "|#"), (";", "\n")]);
    
    // Lua
    insert("lua", vec![("--[[", "]]"), ("--", "\n")]);
    
    // MATLAB
    insert("m", vec![("%{", "%}"), ("%", "\n")]);
    insert("mat", vec![("%{", "%}"), ("%", "\n")]);
    
    // OCaml
    insert("ml", vec![("(*", "*)")]);
    insert("mli", vec![("(*", "*)")]);
    
    // Perl
    insert("pl", vec![("=pod", "=cut"), ("#", "\n")]);
    insert("pm", vec![("=pod", "=cut"), ("#", "\n")]);
    
    // PHP
    insert("php", vec![("/*", "*/"), ("//", "\n"), ("#", "\n")]);
    
    // PowerShell
    insert("ps1", vec![("<#", "#>"), ("#", "\n")]);
    insert("psm1", vec![("<#", "#>"), ("#", "\n")]);
    insert("psd1", vec![("<#", "#>"), ("#", "\n")]);
    
    // Python
    insert("py", vec![("'''", "'''"), ("\"\"\"", "\"\"\""), ("#", "\n")]);
    insert("pyw", vec![("'''", "'''"), ("\"\"\"", "\"\"\""), ("#", "\n")]);
    
    // R
    insert("r", vec![("/*", "*/"), ("#", "\n")]);
    insert("R", vec![("/*", "*/"), ("#", "\n")]);
    
    // Ruby
    insert("rb", vec![("=begin", "=end"), ("#", "\n")]);
    insert("rbw", vec![("=begin", "=end"), ("#", "\n")]);
    
    // Rust
    insert("rs", vec![("/*", "*/"), ("//", "\n")]);
    
    // Scala
    insert("scala", vec![("/*", "*/"), ("//", "\n")]);
    insert("sc", vec![("/*", "*/"), ("//", "\n")]);
    
    // SQL
    insert("sql", vec![("/*", "*/"), ("--", "\n")]);
    
    // Swift
    insert("swift", vec![("/*", "*/"), ("//", "\n")]);
    
    // TypeScript
    insert("ts", vec![("/*", "*/"), ("//", "\n")]);
    insert("tsx", vec![("/*", "*/"), ("//", "\n")]);
    
    // VB.NET
    insert("vb", vec![("'''", "'''"), ("'", "\n")]);
    
    // Infrastructure as Code and Configuration Files
    
    // HCL (Terraform)
    insert("tf", vec![("/*", "*/"), ("#", "\n")]);
    insert("tfvars", vec![("#", "\n")]);
    insert("hcl", vec![("/*", "*/"), ("#", "\n")]);
    
    // YAML files (including various YAML-based configs)
    let yaml_comments = vec![("#", "\n")];
    insert("yaml", yaml_comments.clone());
    insert("yml", yaml_comments.clone());
    insert("docker-compose.yml", yaml_comments.clone());
    insert("docker-compose.yaml", yaml_comments.clone());
    insert("workflow", yaml_comments.clone());
    insert("github-action", yaml_comments.clone());
    insert("circleci", yaml_comments.clone());
    insert(".circleci", yaml_comments.clone());
    
    // Configuration files
    let hash_comments = vec![("#", "\n")];
    insert("dockerfile", hash_comments.clone());
    insert("containerfile", hash_comments.clone());
    insert("nginx", hash_comments.clone());
    insert("htaccess", hash_comments.clone());
    insert("apache2.conf", hash_comments.clone());
    insert("httpd.conf", hash_comments.clone());
    
    // INI and Properties
    insert("ini", vec![(";", "\n")]);
    insert("cfg", vec![(";", "\n")]);
    insert("conf", vec![(";", "\n"), ("#", "\n")]);
    insert("properties", vec![("#", "\n")]);
    insert("prop", vec![("#", "\n")]);
    
    // Infrastructure as Code - JSON-based
    let json_comments = vec![("//", "\n")];
    insert("json", json_comments.clone());
    insert("arm.json", json_comments.clone());
    insert("cf.json", json_comments.clone());
    
    // Configuration Management
    insert("pp", vec![("/*", "*/"), ("#", "\n")]);
    insert("puppet", vec![("#", "\n")]);
    insert("sls", vec![("#", "\n")]);
    insert("salt", vec![("#", "\n")]);
    
    // Modern IaC
    insert("bicep", vec![("/*", "*/"), ("//", "\n")]);
    insert("jsonnet", vec![("/*", "*/"), ("//", "\n")]);
    insert("libsonnet", vec![("/*", "*/"), ("//", "\n")]);
    
    // CI/CD
    insert("jenkinsfile", vec![("/*", "*/"), ("//", "\n")]);
    insert("Jenkinsfile", vec![("/*", "*/"), ("//", "\n")]);
    
    map
}

File Content of ./src/cli.rs:

use clap::Parser;

#[derive(Parser)]
#[command(author, version, about, long_about = None)]
pub struct Cli {
    /// Comma-separated list of file extensions to process (e.g., "txt,md,rs")
    pub suffixes: String,

    /// Path to prompt template file
    #[arg(long)]
    pub prompt_template_path: Option<String>,

    /// Path to output path
    #[arg(long)]
    pub output_path: Option<String>,

    /// Include files that are ignored by default based on .gitignore rules
    #[arg(long, default_value_t = false)]
    pub dont_use_gitignore: bool,

    /// Summarize file contents with keywords
    #[arg(long, default_value_t = false)]
    pub summarize: bool,

    /// Summarize file contents with keywords
    #[arg(long, default_value_t = false)]
    pub summarize_keywords: bool,



    /// Apply summaries to code files
    #[arg(long, default_value_t = false)]
    pub apply: bool,

    /// Retrieve summaries from code files
    #[arg(long, default_value_t = false)]
    pub retrieve: bool,

    /// Comma-separated list of paths to exclude
    #[arg(long)]
    pub exclude_paths: Option<String>,

    /// Comma-separated list of paths to include
    #[arg(long)]
    pub include_paths: Option<String>,

    /// Comma-separated list of keywords - only include files containing at least one keyword
    #[arg(long)]
    pub or_keywords: Option<String>,

    /// Comma-separated list of keywords - only include files containing all keywords
    #[arg(long)]
    pub and_keywords: Option<String>,

    /// Comma-separated list of keywords - exclude files containing any of these keywords
    #[arg(long)]
    pub exclude_keywords: Option<String>,

    /// Only show files that have differences
    #[arg(long, default_value_t = false)]
    pub diff_only: bool,

    /// Starting commit hash for diff comparison
    #[arg(long)]
    pub start_commit_id: Option<String>,

    /// Ending commit hash for diff comparison
    #[arg(long)]
    pub end_commit_id: Option<String>,
}

File Content of ./src/summary.rs:

use reqwest::{Client, header};
use serde::{Deserialize, Serialize};
use tokio::time::{sleep, Duration};
use anyhow::{Result, Context};
use std::env;
use std::path::Path;
use std::collections::HashMap;
use tokio::sync::Semaphore;
use std::sync::Arc;
use std::str::FromStr;
use crate::file_processing::filter_dirscribe_sections;

const DEFAULT_CONCURRENT_REQUESTS: usize = 10;
const ANTHROPIC_MAX_TOKENS: i32 = 512;
const ANTHROPIC_TEMPERATURE: f32 = 0.1;
const MAX_RETRIES: u32 = 6;
const INITIAL_BACKOFF_MS: u64 = 1000;

const DEFAULT_DEEPSEEK_MODEL: &str = "deepseek-chat";
const DEFAULT_ANTHROPIC_MODEL: &str = "claude-3-sonnet-20240229";
const DEFAULT_OLLAMA_MODEL: &str = "deepseek-r1:8b";
const DEFAULT_GEMINI_MODEL: &str = "gemini-1.5-flash";

#[derive(Debug, Clone, Copy)]
pub enum Provider {
    Deepseek,
    Anthropic,
    Ollama,
    Gemini,
}

// Implement FromStr for Provider to parse environment variable
impl FromStr for Provider {
    type Err = anyhow::Error;

    fn from_str(s: &str) -> Result<Self, Self::Err> {
        match s.to_lowercase().as_str() {
            "deepseek" => Ok(Provider::Deepseek),
            "anthropic" => Ok(Provider::Anthropic),
            "ollama" => Ok(Provider::Ollama),
            "gemini" => Ok(Provider::Gemini),
            _ => Err(anyhow::anyhow!("Invalid provider: {}. Valid options are: deepseek, anthropic, ollama", s))
        }
    }
}

// Common message structure used across providers
#[derive(Debug, Serialize, Deserialize, Clone)]
pub struct Message {
    pub role: String,
    pub content: String,
}

// Unified response structure
#[derive(Debug)]
pub struct UnifiedResponse {
    pub content: String,
}

pub struct UnifiedClient {
    client: Client,
    provider: Provider,
    api_key: String,
    base_url: String,
    model: String,
}

impl UnifiedClient {
    pub fn new(provider: Provider) -> Result<Self> {
        let client = Client::new();
        

        let (api_key, base_url, model) = match provider {
            Provider::Deepseek => {
                let key = env::var("PROVIDER_API_KEY")
                    .context("PROVIDER_API_KEY not set")?;
                let model = env::var("DIRSCRIBE_MODEL")
                    .unwrap_or_else(|_| DEFAULT_DEEPSEEK_MODEL.to_string());
                (
                    key,
                    "https://api.deepseek.com/v1/chat/completions".to_string(),
                    model,
                )
            }
            Provider::Anthropic => {
                let key = env::var("PROVIDER_API_KEY")
                    .context("PROVIDER_API_KEY not set")?;
                let model = env::var("DIRSCRIBE_MODEL")
                    .unwrap_or_else(|_| DEFAULT_ANTHROPIC_MODEL.to_string());
                (
                    key,
                    "https://api.anthropic.com/v1/messages".to_string(),
                    model,
                )
            }
            Provider::Ollama => {
                let model = env::var("DIRSCRIBE_MODEL")
                    .unwrap_or_else(|_| DEFAULT_OLLAMA_MODEL.to_string());
                (
                    String::new(), // No API key needed for local Ollama
                    "http://localhost:11434/api/generate".to_string(),
                    model,
                )
            }
            Provider::Gemini => {
                let key = env::var("PROVIDER_API_KEY")
                    .context("PROVIDER_API_KEY not set")?;
                let model = env::var("DIRSCRIBE_MODEL")
                    .unwrap_or_else(|_| DEFAULT_GEMINI_MODEL.to_string());
                (
                    key,
                    format!("https://generativelanguage.googleapis.com/v1beta/models/{}:generateContent", model),
                    model,
                )
            }
        };

        Ok(Self {
            client,
            provider,
            api_key,
            base_url,
            model,
        })
    }

    fn build_headers(&self) -> Result<header::HeaderMap> {
        let mut headers = header::HeaderMap::new();
        
        match self.provider {
            Provider::Deepseek => {
                headers.insert(
                    "Authorization",
                    format!("Bearer {}", self.api_key).parse().unwrap(),
                );
            }
            Provider::Anthropic => {
                headers.insert(
                    "x-api-key",
                    self.api_key.parse().unwrap(),
                );
                headers.insert(
                    "anthropic-version",
                    "2023-06-01".parse().unwrap(),
                );
            }
            Provider::Ollama => {}
            Provider::Gemini => {
                headers.insert(
                    "Content-Type",
                    "application/json; charset=utf-8".parse().unwrap(),
                );
                headers.insert(
                    "X-goog-api-key",
                    self.api_key.parse().unwrap(),
                );
            }
        }
        
        headers.insert(
            "Content-Type",
            "application/json".parse().unwrap(),
        );
        
        Ok(headers)
    }

    fn build_request(&self, messages: Vec<Message>, temperature: Option<f32>, max_tokens: Option<i32>) -> serde_json::Value {
        match self.provider {
            Provider::Deepseek => {
                serde_json::json!({
                    "model": self.model,
                    "messages": messages,
                    "temperature": temperature,
                    "max_tokens": max_tokens,
                    "stream": false
                })
            }
            Provider::Anthropic => {
                serde_json::json!({
                    "model": self.model,
                    "messages": messages,
                    "max_tokens": ANTHROPIC_MAX_TOKENS,
                    "temperature": ANTHROPIC_TEMPERATURE
                })
            }
            Provider::Ollama => {
                // For Ollama, we'll concatenate all messages into a single prompt
                let prompt = messages.iter()
                    .map(|m| format!("{}: {}", m.role, m.content))
                    .collect::<Vec<_>>()
                    .join("\n");
                
                serde_json::json!({
                    "model": self.model,
                    "prompt": prompt,
                    "stream": false
                })
            }
            Provider::Gemini => {
                // Convert messages to Gemini format
                let contents = messages.iter().map(|m| {
                    serde_json::json!({
                        "parts": [{
                            "text": m.content
                        }]
                    })
                }).collect::<Vec<_>>();

                serde_json::json!({
                    "contents": contents,
                    "generationConfig": {
                        "temperature": temperature.unwrap_or(0.7),
                        "maxOutputTokens": max_tokens.unwrap_or(2048)
                    }
                })
            }
        }
    }

    async fn parse_response(&self, response_text: String) -> Result<UnifiedResponse> {
        match self.provider {
            Provider::Deepseek => {
                #[derive(Debug, Deserialize)]
                struct DeepseekResponse {
                    choices: Vec<DeepseekChoice>,
                    #[allow(dead_code)]
                    usage: DeepseekUsage,
                }
                
                #[derive(Debug, Deserialize)]
                struct DeepseekChoice {
                    message: Message,
                }
                
                #[derive(Debug, Deserialize)]
                #[allow(dead_code)]
                struct DeepseekUsage {
                    total_tokens: i32,
                }

                let response: DeepseekResponse = serde_json::from_str(&response_text)?;
                Ok(UnifiedResponse {
                    content: response.choices[0].message.content.clone()
                })
            }
            Provider::Anthropic => {
                #[derive(Debug, Deserialize)]
                struct AnthropicResponse {
                    content: Vec<AnthropicContent>,
                    #[allow(dead_code)]
                    usage: AnthropicUsage,
                }
                
                #[derive(Debug, Deserialize)]
                struct AnthropicContent {
                    #[serde(rename = "type")]
                    #[allow(dead_code)]
                    content_type: String,
                    #[serde(rename = "text")]
                    message: String,
                }
                
                #[derive(Debug, Deserialize)]
                #[allow(dead_code)]
                struct AnthropicUsage {
                    input_tokens: i32,
                    output_tokens: i32,
                }

                let response: AnthropicResponse = serde_json::from_str(&response_text)?;
                Ok(UnifiedResponse {
                    content: response.content[0].message.clone()
                })
            }
            Provider::Ollama => {
                #[derive(Debug, Deserialize)]
                struct OllamaResponse {
                    response: String,
                    #[allow(dead_code)]
                    done: bool,
                }
                let response: OllamaResponse = serde_json::from_str(&response_text)?;
                let content = if response.response.contains("</think>") {
                    response.response
                        .split("</think>")
                        .nth(1)
                        .unwrap_or(&response.response)
                        .trim()
                        .to_string()
                } else {
                    response.response.clone()
                };
                
                Ok(UnifiedResponse {
                    content
                })
            }
            Provider::Gemini => {
                #[derive(Debug, Deserialize)]
                struct GeminiResponse {
                    candidates: Vec<GeminiCandidate>,
                }

                #[derive(Debug, Deserialize)]
                struct GeminiCandidate {
                    content: GeminiContent,
                }

                #[derive(Debug, Deserialize)]
                struct GeminiContent {
                    parts: Vec<GeminiPart>,
                }

                #[derive(Debug, Deserialize)]
                struct GeminiPart {
                    text: String,
                }

                let response: GeminiResponse = serde_json::from_str(&response_text)?;
                
                // Get the first candidate's text
                let content = response.candidates
                    .first()
                    .ok_or_else(|| anyhow::anyhow!("No response candidates"))?
                    .content
                    .parts
                    .first()
                    .ok_or_else(|| anyhow::anyhow!("No response parts"))?
                    .text
                    .clone();

                Ok(UnifiedResponse { content })
            }
        }
    }

    pub async fn chat(&self, suffix_map: &HashMap<&'static str, Vec<(&'static str, &'static str)>>, diff_only: bool,  file_path: &str, messages: &Vec<Message>, temperature: Option<f32>, max_tokens: Option<i32>) -> Result<UnifiedResponse> {
        let request = self.build_request(messages.clone(), temperature, max_tokens);
        let headers = self.build_headers()?;
        
        let mut retries = 0;
        let mut backoff_ms = INITIAL_BACKOFF_MS;
    
        loop {
            let response = self.client
                .post(&self.base_url)
                .headers(headers.clone())
                .json(&request)
                .send()
                .await?;
    
            let status = response.status();
            let response_text = response.text().await?;
            
            // First check if the request was successful
            if status.is_success() {
                // Try to parse the response
                match self.parse_response(response_text.clone()).await {
                    Ok(parsed_response) => {
                        // Check if the summary is valid
                        let summary_format_correct = check_summary(Path::new(file_path), &parsed_response.content, suffix_map);
                        if diff_only | summary_format_correct {
                            return Ok(parsed_response);
                        } else {
                            // If summary validation fails, treat it like a retriable error
                            if retries >= MAX_RETRIES {
                                return Ok(parsed_response); //return badly formatted sumnmary rather than nothing
                            }
                            // Continue to retry logic
                        }
                    }
                    Err(e) => {
                        // If parsing fails and we're out of retries, bail
                        if retries >= MAX_RETRIES {
                            anyhow::bail!("Failed to parse response after {} retries: {}", MAX_RETRIES, e);
                        }
                        // Continue to retry logic
                    }
                }
            } else if !status.is_server_error() && status != 429 {
                // Only bail immediately on non-retriable errors
                anyhow::bail!("API request failed with non-retriable error: {}", response_text);
            }
    
            // Retry logic
            if retries >= MAX_RETRIES {
                anyhow::bail!("Max retries exceeded. Last error: {} {}", status, response_text);
            }
    
            sleep(Duration::from_millis(backoff_ms)).await;
            retries += 1;
            backoff_ms *= 2;
        }
    }
}

pub async fn get_summaries(
    valid_files: Vec<String>, 
    file_contents: HashMap<String, String>, 
    prompt_template: String,
    suffix_map: HashMap<&'static str, Vec<(&'static str, &'static str)>>,
    diff_only:bool
) -> Result<Vec<String>> {
    // Get provider from environment variable, default to Ollama if not set
    let provider = env::var("DIRSCRIBE_PROVIDER")
        .map(|p| Provider::from_str(&p))
        .unwrap_or(Ok(Provider::Ollama))?;

    let client = Arc::new(UnifiedClient::new(provider)?);
    let max_concurrent_requests: usize =  env::var("DIRSCRIBE_CONCURRENT_REQUESTS").unwrap_or_else(|_| DEFAULT_CONCURRENT_REQUESTS.to_string()).parse().unwrap_or(DEFAULT_CONCURRENT_REQUESTS);

    let semaphore = Arc::new(Semaphore::new(max_concurrent_requests));
    let suffix_map = Arc::new(suffix_map);
    
    // Rest of the function remains the same
    let mut handles = Vec::new();
    
    for file_path in valid_files {
        let permit = semaphore.clone().acquire_owned().await?;
        let content = file_contents.get(&file_path).unwrap_or(&String::new()).clone();
        let processed_content = filter_dirscribe_sections(&content, true);
        let file_path_clone = file_path.clone();
        let client = client.clone();
        let suffix_map = Arc::clone(&suffix_map);
        let prompt_template = prompt_template.clone();

        let extension = Path::new(&file_path)
            .extension()
            .and_then(|ext| ext.to_str())
            .unwrap_or(""); 

        let prompt_base = prompt_template.replace("${${CONTENT}$}$", &processed_content);
        let prompt = if !diff_only {
            if let Some(comment_chars) = suffix_map.get(extension)  {
                let (multi_line_comment_start, multi_line_comment_end) = comment_chars[0];
                 
                if multi_line_comment_end != "single line" {
                    prompt_base.to_owned() + &format!("\n\nPlease use the following structure: line 1: '{}', line 2: '[DIRSCRIBE]', lines 3 to N -2: *the summary*, line N-1: '[/DIRSCRIBE]', line N: '{}'. The appropriate start and end of a multiline comment are '{}' and '{}', respectively.", 
                        multi_line_comment_start, multi_line_comment_end, multi_line_comment_start, multi_line_comment_end)
                } else {
                    prompt_base.to_owned() + &format!("\n\nPlease make sure to start every line of the summary with '{}'. Please use the following structure: line 1: '{}', line 2: '{} [DIRSCRIBE]', lines 3 to N -2: *the summary*, line N-1: '{} [/DIRSCRIBE]', line N: '{}'. The appropriate start and end of a multiline comment are '{}' and '{}', respectively.", 
                        multi_line_comment_start, multi_line_comment_start, multi_line_comment_start, multi_line_comment_start, multi_line_comment_start, multi_line_comment_start, multi_line_comment_end)
                }
            } else {
                prompt_base.to_owned() + &"\n\nPlease make sure to return the summary as a comment block appropriately formatted for the language, with this structure: line 1: , line 2: [DIRSCRIBE], line N-1: [/DIRSCRIBE], line N: . Lines 1 and N should be empty."
            }
        } else {
            prompt_base.to_string()
        };

        let messages: Vec<Message> = vec![Message {
            role: "user".to_string(),
            content: prompt,
        }];

        let handle = tokio::spawn(async move {
            let result = client.chat(&suffix_map, diff_only, &file_path_clone, &messages, None, None).await;
            drop(permit);
            match result {
                Ok(response) => Ok(response.content),
                Err(e) => Err(anyhow::anyhow!("Error processing file {}: {}", file_path_clone, e))
            }
        });
        
        handles.push(handle);
    }
    
    let mut results = Vec::new();
    for handle in handles {
        match handle.await? {
            Ok(content) => results.push(content),
            Err(e) => results.push(format!("Error: {}", e)),
        }
    }
    Ok(results)
}

pub fn check_summary(file_path: &Path, s: &str, suffix_map: &HashMap<&'static str, Vec<(&'static str, &'static str)>>) -> bool {
    let extension = file_path.extension()
        .and_then(|ext| ext.to_str())
        .unwrap_or(""); 
    if let Some(comment_chars) = suffix_map.get(extension) {
        for (multi_line_comment_start, multi_line_comment_end) in comment_chars {
            let lines: Vec<&str> = s.trim().split('\n').collect();
            if lines.len() < 4 {
                continue;
            }
            
            if *multi_line_comment_end != "\n" {
                let comment_start = lines[0].trim() == *multi_line_comment_start;
                let dirscribe_start = lines[1].trim() == "[DIRSCRIBE]";
                let dirscribe_end = lines[lines.len() - 2].trim() == "[/DIRSCRIBE]";
                let comment_end = lines[lines.len() - 1].trim() == *multi_line_comment_end;
                
                if comment_start && dirscribe_start && dirscribe_end && comment_end {
                    return true;
                }
            } else {
                let comment_start = lines[0].trim() == *multi_line_comment_start;
                let dirscribe_start = lines[1].trim() == format!("{} [DIRSCRIBE]", multi_line_comment_start);
                let dirscribe_end = lines[lines.len() - 2].trim() == format!("{} [/DIRSCRIBE]", multi_line_comment_start);
                let comment_end = lines[lines.len() - 1].trim() == *multi_line_comment_start;
                
                if comment_start && dirscribe_start && dirscribe_end && comment_end {
                    return true;
                }
            }
        }
    }
    false
}

